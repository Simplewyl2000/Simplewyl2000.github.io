<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Naive Bayes &amp; Bayes</title>
    <link href="/2020/03/08/Naive%20Bayes%20&amp;%20Bayes/"/>
    <url>/2020/03/08/Naive%20Bayes%20&amp;%20Bayes/</url>
    
    <content type="html"><![CDATA[<h1 id="Naive-Bayes-amp-Bayes"><a href="#Naive-Bayes-amp-Bayes" class="headerlink" title="Naive Bayes &amp; Bayes"></a>Naive Bayes &amp; Bayes</h1><p>The difference between naive Bayes and Bayes is that naive Bayes assumes that the value of the input feature is independent distributed.</p><p>This a quite strong assumption, and that’s why it is named as naive Bayes.</p><h4 id="Notation-explanation-some-notations-that-will-be-used-in-the-following-blog"><a href="#Notation-explanation-some-notations-that-will-be-used-in-the-following-blog" class="headerlink" title="Notation explanation(some notations that will be used in the following blog)"></a>Notation explanation(some notations that will be used in the following blog)</h4><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216005215134.png" srcset="/img/loading.gif" alt="image-20200216005215134"> means that the j^th feature of the i^th  input.</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216005416391.png" srcset="/img/loading.gif" alt="image-20200216005416391"> means the l^th possible value of the  j^th feature</p><h3 id="Learning-Process"><a href="#Learning-Process" class="headerlink" title="Learning Process"></a>Learning Process</h3><p>The learning process for the Bayes is to ‘learn’ the prior probability and the condition probability.</p><p><strong><em>prior</em></strong> <strong><em>probability</em></strong>:  probability of a certain deed to happen</p><p>Say, something like this:</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216004257349.png" srcset="/img/loading.gif" alt="image-20200216004257349"></p><p>Condition probability:</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216004338052.png" srcset="/img/loading.gif" alt="image-20200216004338052"></p><p>Because the naive Bayes assume that all the feature of one input are independent, so the above formula can be delivered in another way:</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216004536707.png" srcset="/img/loading.gif" alt="image-20200216004536707"></p><p>After ‘learn’  the above parameter, then we begin the classification process</p><h3 id="Classification-Process"><a href="#Classification-Process" class="headerlink" title="Classification Process"></a>Classification Process</h3><p>The aim of Bayes model is to classify the input samples into certain categories.</p><p>which means:</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216010800035.png" srcset="/img/loading.gif" alt="image-20200216010800035"></p><p>Intuitively, for all the probability we derive from the formula below, we choose the class which get the max value as the output of x. (Note that the condition probability and the prior probability have been ‘learned’)</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216011137636.png" srcset="/img/loading.gif" alt="image-20200216011137636"></p><p>And according to the basic assumption of the naive Bayes model the formula can also be written in this way:</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216011427823.png" srcset="/img/loading.gif" alt="image-20200216011427823"></p><p>Then we choose the class for x:</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216011530347.png" srcset="/img/loading.gif" alt="image-20200216011530347"></p><h3 id="How-to-‘learn’-the-parameter"><a href="#How-to-‘learn’-the-parameter" class="headerlink" title="How to ‘learn’ the parameter?"></a>How to ‘learn’ the parameter?</h3><p>We use the MLE (Maximum Likelihood Estimate)</p><p>Because the input and the output are discrete. So we can’t use the  derivative here. </p><p>I leave out the proof  of the following formulas here, those who are interested in that can try it yourself. And I will explain the following formulas in a simpler way</p><p>For the prior probability, we simply count the time of the class  appear in the training dataset, because using this method the parameter will fit the current dataset well. </p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216012308925.png" srcset="/img/loading.gif" alt="image-20200216012308925"></p><p>And the same thing for the condition probability</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216012333606.png" srcset="/img/loading.gif" alt="image-20200216012333606"></p><p>This learning process is quite simple as you see above. It’s funny to call it a learning process., so I add quotation marks to the word learn. XD</p><h3 id="Bayes-model"><a href="#Bayes-model" class="headerlink" title="Bayes model"></a>Bayes model</h3><p>It is easy to for us to notice a problem from the above method that we perfunctorily assume that classes that haven’t appear in the dataset have the zero probability to happen!</p><p> Therefore we introduce the Laplace Smoothing:</p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216014417115.png" srcset="/img/loading.gif" alt="image-20200216014417115"></p><p><img src="https://raw.githubusercontent.com/Simplewyl2000/blogImg/master/image-20200216014427826.png" srcset="/img/loading.gif" alt="image-20200216014427826"></p><p>In these two equations, for those classes that haven’t appear in the dataset, the probability will  not be zero.   </p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
