<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="description" content="">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <title>Intermediate representation embedding investigation - Simple</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/5.12.1/css/all.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/mdbootstrap/4.13.0/css/mdb.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />

<link  rel="stylesheet" href="/css/main.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />


<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Simple</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" false
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <p class="mt-3 post-meta">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>
                  Saturday, November 28th 2020, 4:17 pm
                </p>
              

              <p class="mt-1">
                
                  
                  <span class="post-meta">
                    <i class="far fa-chart-bar"></i>
                    2.4k 字
                  </span>
                

                
                  
                  <span class="post-meta">
                      <i class="far fa-clock"></i>
                      9 分钟
                  </span>
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  <span id="busuanzi_container_page_pv" class="post-meta" style="display: none">
                    <i class="far fa-eye" aria-hidden="true"></i>
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          
          <div class="markdown-body">
            <h1 id="Intermediate-representation-embedding-investigation"><a href="#Intermediate-representation-embedding-investigation" class="headerlink" title="Intermediate representation embedding investigation"></a>Intermediate representation embedding investigation</h1><p>在这里整理了大体上两类embedding的方法：非基于GNN的，基于GNN模型的</p>
<p>分别介绍了相关的五篇文章，其中前两篇比较相近也有对比实验，第三第四篇比较相近，之前这四篇都是基于LLVM-IR的embedding，最后一篇是基于AST的embedding。</p>
<p>最后对方向做出了自己的感想和总结。</p>
<h2 id="IR2Vec-LLVM-IR-based-Scalable-Program-Embeddings"><a href="#IR2Vec-LLVM-IR-based-Scalable-Program-Embeddings" class="headerlink" title="IR2Vec: LLVM IR based Scalable Program Embeddings"></a><strong>IR2Vec: LLVM IR based Scalable Program Embeddings</strong></h2><p>指出，目前的程序表示可以大致分为两种</p>
<p>一种是feature-based: 这个应该跟付才之前做的那个有点类似</p>
<p>一种是distributed的：这个就是learning-based的了，用的一般是NLP的方法或者是GNN的方法。</p>
<p>对于无监督的distributed vector的embedding， 又大致分为两种：</p>
<p>(1) Context-window based embedding: Methods such as Word2Vec , GloVe  fall under this category.</p>
<p>(2) Knowledge graph embedding: Methods such as TransE, TransR , TransD, TransH fall under this category.</p>
<p>Embedding的应用场景</p>
<ul>
<li><p>suggesting meaningful method names </p>
<ul>
<li><p><em>Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. Suggesting accurate method and class names. In Proc. of the 2015 10th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2015, pages. 38–49, USA, 2015. ACM</em></p>
</li>
<li><p><em>Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. Code2vec: Learning distributed representations of code. Proc. ACM Program. Lang., 3(POPL):40:1–40:29, January 2019.</em></p>
</li>
</ul>
</li>
<li><p>Solve the optimization problems like thread coarsening and device mapping in OpenCL</p>
<ul>
<li><p><em>Chris Cummins, Pavlos Petoumenos, Zheng Wang, and Hugh Leather. End-to-end deep learning of optimization heuristics. In 2017 26th International Conference on Parallel Architectures and Compilation Techniques</em></p>
<p><em>(PACT), pages 219–232. IEEE, 2017.</em></p>
</li>
</ul>
</li>
<li><p>program classification, code search, prediction of vectorization/unroll factors, similarity detection</p>
</li>
</ul>
<p>本篇文章提出的inst2vec方法。</p>
<p>这里简述一下：</p>
<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><p><img src="https://i.loli.net/2020/11/13/HSb1mAaK3RQhei8.png" srcset="/img/loading.gif" alt="image-20201113141326364"></p>
<h3 id="产生seed-embedding"><a href="#产生seed-embedding" class="headerlink" title="产生seed embedding"></a>产生seed embedding</h3><p>​    因为是使用知识图谱作为embedding的方法，知识图谱一般是由三元组来表示的，这里也是一样。</p>
<p>​    知识图谱: <a href="https://zhuanlan.zhihu.com/p/31726910" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31726910</a></p>
<p>​    这里是：&lt;h, r, t&gt; 分别代表opcode，关系，对象。</p>
<p>​    举个例子来说：<br>​    <img src="https://i.loli.net/2020/11/13/jRItbq2nSzVDYEr.png" srcset="/img/loading.gif" alt="image-20201113142529522"></p>
<h3 id="在产生seed-embedding之后，之后就是对instruction做embedding"><a href="#在产生seed-embedding之后，之后就是对instruction做embedding" class="headerlink" title="在产生seed embedding之后，之后就是对instruction做embedding"></a>在产生seed embedding之后，之后就是对instruction做embedding</h3><p>这里用的是一个公式：</p>
<p><img src="https://i.loli.net/2020/11/18/hgIJN5SfDnPqEWx.png" srcset="/img/loading.gif" alt="image-20201113142818033"></p>
<p>其中W（0-1）都是标量</p>
<p><strong>注意其中标黄的部分。</strong></p>
<p><img src="https://i.loli.net/2020/11/18/hgIJN5SfDnPqEWx.png" srcset="/img/loading.gif" alt="image-20201113143002589"></p>
<p>第一个公式中的A是这么计算来的，RD是reach definition， 也就是说，在第一个公式中的A，某个指令的操作数，并不只是将其的TYPE（比如PTR）作为A，而是将所有reach 到这里的definition 的statement的embedding加起来的。</p>
<p>举个例子</p>
<p><img src="https://i.loli.net/2020/11/18/hgIJN5SfDnPqEWx.png" srcset="/img/loading.gif" alt="image-20201113143959129"></p>
<p><img src="https://i.loli.net/2020/11/18/hgIJN5SfDnPqEWx.png" srcset="/img/loading.gif" alt="image-20201113144035061"></p>
<p>对于循环依赖，解方程就可以了。</p>
<h3 id="BB-amp-FUNCTION-amp-PROGRAM"><a href="#BB-amp-FUNCTION-amp-PROGRAM" class="headerlink" title="BB &amp; FUNCTION &amp; PROGRAM"></a>BB &amp; FUNCTION &amp; PROGRAM</h3><p>对于BB来说就是BB内有效指令的求和，同理对function和program</p>
<p><img src="https://i.loli.net/2020/11/13/3hmSBZvXs4R7GF2.png" srcset="/img/loading.gif" alt="image-20201113144145685"></p>
<p><img src="https://i.loli.net/2020/11/13/2MlqIEVGzFWhScd.png" srcset="/img/loading.gif" alt="image-20201113144246622"></p>
<p><img src="https://i.loli.net/2020/11/13/n2MVsqOiDTZa4Sk.png" srcset="/img/loading.gif" alt="image-20201113144257984"></p>
<h3 id="More"><a href="#More" class="headerlink" title="More"></a>More</h3><ol>
<li>感觉对于BB和function以及program的表示太简单了一点，这样损失信息挺严重的感觉。</li>
<li>相对于其他很多embedding的方法，比如 Context-window based embedding，这里用的是Knowledge graph embedding，但是这两者在这方面的应用性能差异还有待考究。</li>
</ol>
<h2 id="Neural-Code-Comprehension-A-Learnable-Representation-of-Code-Semantics"><a href="#Neural-Code-Comprehension-A-Learnable-Representation-of-Code-Semantics" class="headerlink" title="Neural Code Comprehension: A Learnable Representation of Code Semantics"></a><strong>Neural Code Comprehension: A Learnable</strong> Representation of Code Semantics</h2><p>这篇文章自称是第一篇做LLVM-IR embedding的。</p>
<blockquote>
<p>To the best of our knowledge, however, no attempt has been made to train embeddings for compiler IRs prior to this work.</p>
</blockquote>
<p>可以看一下总体结构：</p>
<p><img src="https://i.loli.net/2020/11/11/X2fRuNyKTtrcC8F.png" srcset="/img/loading.gif" alt="image-20201111171255627"></p>
<p>很简单的思路，就是一个正常流程，但是作者好像为了使得自己的成果fancy一些加了一个叫做XFG的东西。</p>
<p>XFG的构建流程就是一个data-flow，但是考虑到了一些执行上的上下文比如lable。</p>
<p>详见：</p>
<p><img src="https://i.loli.net/2020/11/11/VvpAinQWLMy6Sl5.png" srcset="/img/loading.gif" alt="image-20201111171334843"></p>
<p>效果如下所示:</p>
<p><img src="https://i.loli.net/2020/11/11/V3zNElCBvIXZgYp.png" srcset="/img/loading.gif" alt="image-20201111171318539"></p>
<p>之后训练这一部分讲的很简单，因为是直接套用了别人的模型：skip-gram, 只是说了一下statement上下文的提取，这里就用到了之前做的XFG。</p>
<p>实验效果对比：</p>
<p>对于openCL的优化，accuracy：</p>
<p><img src="https://i.loli.net/2020/11/18/hgIJN5SfDnPqEWx.png" srcset="/img/loading.gif" alt="image-20201114170438826"></p>
<p>以及speed up，这里就不放了。</p>
<p><strong>以下是图有关的</strong></p>
<h2 id="PROGRAML-GRAPH-BASED-DEEP-LEARNING-FOR-PROGRAM-OPTIMIZATION-AND-ANALYSIS"><a href="#PROGRAML-GRAPH-BASED-DEEP-LEARNING-FOR-PROGRAM-OPTIMIZATION-AND-ANALYSIS" class="headerlink" title="PROGRAML: GRAPH-BASED DEEP LEARNING FOR PROGRAM OPTIMIZATION AND ANALYSIS"></a>PROGRAML: GRAPH-BASED DEEP LEARNING FOR PROGRAM OPTIMIZATION AND ANALYSIS</h2><p>这篇文章和之前那个是一个实验室做的，都是ETH的实验室。</p>
<blockquote>
<p>The PROGRAML representation of a compiler IR serves as the union between a call graph, control-flow graph, and data-flow graph.</p>
</blockquote>
<p><img src="https://i.loli.net/2020/11/17/IjPAJbqKdHimwFL.png" srcset="/img/loading.gif" alt="image-20201117155642564"></p>
<ul>
<li>Control flow:<ul>
<li>在有多个分支的时候，每个分支被标记了顺序</li>
</ul>
</li>
<li>Data flow:<ul>
<li>每个变量以及常数(oprands)都会是一个节点</li>
<li>会标记变量类型，以示区别，同时也会区别开各个不同作用域的变量</li>
<li>操作数的顺序也是被标记的</li>
</ul>
</li>
<li>call graph：<ul>
<li>正常的call graph， 如果是比如动态连接不在当前可以遍历到的IR内，就以一个假节点(external call)代替调用.</li>
</ul>
</li>
</ul>
<p>在得到图之后，首先这里扩展了inst2vec的vocabulary，得到基本IR的mapping，然后使用了</p>
<p> <strong>Message Passing Neural Network (MPNN) framework</strong>，得到图的embedding。</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li>结合了data flow， control flow， 调用关系，比之前的方法更加完善的表示了程序的语义信息</li>
<li>考虑到了操作数之间的优先级，以及branch之间的顺序等，更好得去帮助embedding。</li>
</ol>
<h2 id="Compiler-Based-Graph-Representations-for-Deep-Learning-Models-of-Code"><a href="#Compiler-Based-Graph-Representations-for-Deep-Learning-Models-of-Code" class="headerlink" title="Compiler-Based Graph Representations for Deep Learning Models of Code"></a><strong>Compiler-Based Graph Representations for Deep Learning Models of Code</strong></h2><p>这篇文章质量不高</p>
<p>首先介绍了几种图表示的程序结构</p>
<h3 id="基本序列"><a href="#基本序列" class="headerlink" title="基本序列"></a>基本序列</h3><ol>
<li><p>Tokens 流</p>
<p>对于token流，处理比较简单，直接用RNN/LSTM</p>
</li>
</ol>
<h3 id="图表示"><a href="#图表示" class="headerlink" title="图表示"></a>图表示</h3><ol>
<li><p>拓展的AST</p>
<p>在基本的AST上，加了data-flow的特性，比如use-def</p>
</li>
<li><p>CDFG</p>
<p> control and data flow graph (CDFG)，这个如何构建并不是很复杂的事情，文中也没有细说，只是提到了是data-flow+control-flow加上call edge。</p>
</li>
</ol>
<p>比较重点的部分在图的表示上</p>
<ol>
<li><p>使用Multi-Layer Perceptron (MLP) neural network. 进行基本的embedding，获取token的embedding值</p>
<p><img src="https://i.loli.net/2020/11/17/fw3vnHKV8p4tsFm.png" srcset="/img/loading.gif" alt="image-20201117112203134"></p>
</li>
<li><p>因为这里只是考虑了点的信息，没有考虑边，这里使用了一个information propagation scheme, 使得整个图的信息考虑了进去。</p>
<p>其中的hv就是上面的点的embedding，Aet，bet则是有关et的参数，et是边。</p>
<p><img src="https://i.loli.net/2020/11/17/NGhH7CesZ8LoRMY.png" srcset="/img/loading.gif" alt="image-20201117112801292"></p>
</li>
<li><p>在经历过指定T轮的迭代之后，整个图中所有节点进行aggregate在一起就是图的embedding。</p>
</li>
</ol>
<p><img src="https://i.loli.net/2020/11/18/1kJlG2uOw4MzP7I.png" srcset="/img/loading.gif" alt="image-20201118160410443"></p>
<h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><p>这个CDFG建得还不如上面的几个，还只是考虑了opcode没有考虑oprands.</p>
<h2 id="code2vec-Learning-Distributed-Representations-of-Code"><a href="#code2vec-Learning-Distributed-Representations-of-Code" class="headerlink" title="code2vec: Learning Distributed Representations of Code"></a><strong>code2vec: Learning Distributed Representations of Code</strong></h2><p>这篇文章相对于之前两篇来说质量高了不少</p>
<p>本篇文章的主要目标是利用AST树来对程序切片或者给function命名，用于做code review或者API查找。所以训练的时候也是用的function name的tag来进行训练的。</p>
<p>本文主要是用了path-based attention model对AST来做的，这一点比较新颖。</p>
<p><img src="https://i.loli.net/2020/11/18/Mpd26TLkXF5jul3.png" srcset="/img/loading.gif" alt="image-20201118160826303"></p>
<blockquote>
<p>A path-based attention model for learning vectors for arbitrary-sized snippet of code. This model allows us to embed a program, which is a discrete object, into a continuous space, such that it can be fed into a deep learning pipeline for various tasks.</p>
</blockquote>
<ul>
<li>利用一组context 来表示一个程序</li>
<li>一组如何表示？ aggregated or top？</li>
<li>如何判断context之间的priority</li>
</ul>
<h3 id="如何表示context"><a href="#如何表示context" class="headerlink" title="如何表示context?"></a>如何表示context?</h3><p><img src="https://i.loli.net/2020/11/18/hgIJN5SfDnPqEWx.png" srcset="/img/loading.gif" alt="image-20201117174458908"></p>
<p>以&lt;Xt, p, Xt&gt;的形式来表示，Xt是AST中的叶子结点，p是前一个Xt到后一个AST的路径，表示形式为：</p>
<p>n1d1…nkdknk+1</p>
<p>n就是经过节点，d代表在AST上的移动方向，比如子节点向父节点就是up，父节点向子节点就是down。</p>
<p><img src="https://i.loli.net/2020/11/17/4TH5UONpPnDk6Jb.png" srcset="/img/loading.gif" alt="image-20201117174914736"></p>
<h3 id="context如何encode"><a href="#context如何encode" class="headerlink" title="context如何encode"></a>context如何encode</h3><p>这个比较简单，跟很多模型一样，对于Xt， path都是随机初始化，之后在训练的时候更新。</p>
<p>对于一个context向量就是直接拼接</p>
<p><img src="https://i.loli.net/2020/11/17/dtTCag3bN2GODcs.png" srcset="/img/loading.gif" alt="image-20201117175238868"></p>
<h3 id="如何确定context-vector之间的重要性"><a href="#如何确定context-vector之间的重要性" class="headerlink" title="如何确定context vector之间的重要性"></a>如何确定context vector之间的重要性</h3><p>在全联接层输出之后接了一个attention vector(公式中的a)的参数(其实也不算是特殊接出来的吧)，这个参数也是会随着训练更新的。</p>
<p><img src="https://i.loli.net/2020/11/17/6OyFRINin3YewcX.png" srcset="/img/loading.gif" alt="image-20201117175508211"></p>
<h3 id="如何使用一组context来表示程序"><a href="#如何使用一组context来表示程序" class="headerlink" title="如何使用一组context来表示程序"></a>如何使用一组context来表示程序</h3><p>要么使用aggregated method 要么就 选取top one，这里选取了aggregated的方法（应该大部分选的都是这个方法吧）</p>
<p><img src="https://i.loli.net/2020/11/17/pT8DEGQuoKJvByn.png" srcset="/img/loading.gif" alt="image-20201117175807304"></p>
<h3 id="不足-1"><a href="#不足-1" class="headerlink" title="不足"></a>不足</h3><p>只是单纯的AST的话是没有能抓住更多语义信息的，比如data-flow，control-flow等。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>一开始提出对LLVM-IR的embedding时候，采用的方法是选取了Instructions的上下文，作为一个片段放到skip-gram里面去对IR进行embedding。很显然这个方法对于之后的一些方法来说，没有做到细粒度的embedding. 因此在表示上，可能也会有失精确。</p>
<p>在这之后的中间语言表示大概是这样一个模式： </p>
<p>对中间语言先做预处理 -&gt; 对opcode 和 operands（可选）进行基本的embedding -&gt; 使用前一步的embedding产生instruction的embedding -&gt; 此时通过诸如CDFG等的语义信息，将各个instructions的信息关联起来重新embedding，使用的一般是拓展的GNN（Message Passing Neural Network (MPNN) framework）。</p>
<p>从这个流程来看，如果要提升效果，可以从以下几个方向入手：</p>
<ol>
<li><p><strong>IR的语义表示上</strong></p>
<p>因为整个发展的过程中，可以看出各个实验室，各个方法都是希望能够更精确，更细粒度的去表示IR之间的关系以及IR本身的含义的，包括使用CDFG，甚至保留了IR中operands之间的先后顺序这样。</p>
<p>但是感觉这个方向比较饱和，因为可以看出最近的几篇其实是换汤不换药，基本的大方向没有太大变化，只是在细节上有一些出入（比如对外部调用的处理，内存操作的处理等）。</p>
</li>
<li><p><strong>embedding的方法上</strong></p>
<p>目前的话，embedding主要借用的还是NLP和GNN相关领域的模型，如果希望在这些模型的效果上做提升那就有点南辕北辙(毕竟我们不是做NLP的)，所以这部分embedding方法上肯定还是借用，所以如果在NLP或者GNN领域有新的突破，可以及时关注并联想到这里迁移过来。</p>
</li>
</ol>
<p>所以总体来看感觉在IR embedding本身上并不是很有发展的潜力</p>
<p>但是IR embedding的应用有着天然的优势，就是跨平台，跨语言，所以在IR embedding上应该多投入精力到IR embedding的应用上去，比如漏洞检测，代码相似度检测，静态代码分析，编译优化等。</p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/Deep-learning/">Deep learning</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
              
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/Math/">Math</a>
                
                  <a class="hover-with-bg" href="/tags/Deep-learning/">Deep learning</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container">
        <div id="toc">
  <p class="h5"><i class="far fa-list-alt"></i>&nbsp;TOC</p>
  <div id="tocbot"></div>
</div>
      </div>
    
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>

    
  <div>
    
      <!-- 不蒜子统计PV -->
      
      <span id="busuanzi_container_site_pv" style="display: none">
      总访问量 <span id="busuanzi_value_site_pv"></span> 次
    </span>
    
    
      <!-- 不蒜子统计UV -->
      
      <span id="busuanzi_container_site_uv" style="display: none">
      总访客数 <span id="busuanzi_value_site_uv"></span> 人
    </span>
    
  </div>


    

    <!-- cnzz Analytics icon -->
    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/popper.js/1.16.1/umd/popper.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="https://cdn.staticfile.org/mdbootstrap/4.13.0/js/mdb.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var navHeight = $('#navbar').height();
      var toc = $('#toc');
      var main = $('main');
      var tocT = navHeight + (toc.offset().top - main.offset().top);
      var tocLimMin = main.offset().top - navHeight;
      var tocLimMax = $('#comments').offset().top - navHeight;
      $(window).scroll(function () {
        var scroH = document.body.scrollTop + document.documentElement.scrollTop;
        if (tocLimMin <= scroH && scroH <= tocLimMax) {
          toc.css({
            'display': 'block',
            'position': 'fixed',
            'top': tocT,
          });
        } else if (scroH <= tocLimMin) {
          toc.css({
            'position': '',
            'top': '',
          });
        } else if (scroH > tocLimMax) {
          toc.css('display', 'none');
        }
      });
      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        scrollSmooth: true,
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc > p').css('visibility', 'visible');
      }
    });
  </script>







  <script defer src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




<!-- Plugins -->



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Intermediate representation embedding investigation&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>












</body>
</html>
